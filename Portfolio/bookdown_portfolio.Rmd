---
title: "Portfolio"
author: "Fatima Danawar"
date: "2024"
site: bookdown::bookdown_site
documentclass: book
new_session: yes
output:
  bookdown::gitbook:
    highlight: tango
bibliography: [book.bib]
params:
  country: "Netherlands"
  year: "2024"
  continent: "Europe"

  
  #bookdown::pdf_book: default
---

# Portfolio Inleiding
Welkom bij mijn portfolio! In dit document deel ik een overzicht van mijn datascience-vaardigheden en -projecten. Als aspirant-datascientist heb ik mijn reis vastgelegd door middel van hands-on projecten en praktische ervaringen, met de nadruk op het gebruik van R, SQL en gerelateerde tools.

## Over mij
Mijn naam is Fatima , een enthousiaste datascience-enthousiasteling met een passie voor het begrijpen en analyseren van gegevens om waardevolle inzichten te verkrijgen. Tijdens mijn leertraject heb ik me gericht op het ontwikkelen van vaardigheden op het gebied van gegevensverwerking, analyse en visualisatie.

<!--chapter:end:index.Rmd-->

# Curriculum Vitae

## Mijn CV met `vitae` R package

```{r CV, echo=F, out.width="100%", out.height=1000}
knitr::include_graphics('C:/Users/Laptop/Downloads/Workflow-Portfolio/Portfolio/doc/CV.pdf')
```

## Script
The script I have used for creating this CV can be seen [here](https://github.com/Fatimadanawar/Workflow-Portfolio/blob/main/doc/CV.Rmd) on my github.



<!--chapter:end:01_CV.Rmd-->

---
title: "Leerplan"
author: "Fatima Danawar"
date: "2023-11-21"
output: html_document
---
# Doel: 
Over ~2 jaar wil ik een ervaren professional zijn in het interpreteren van Whole Exome Sequencing (WES) met een specialisatie in het identificeren van genetische oorzaken van zeldzame genetische aandoeningen, met name het KBG-syndroom. Ik wil diepere expertise ontwikkelen in het gebruik van geavanceerde tools zoals GATK om nauwkeurige genetische analyses uit te voeren.

# Huidige Status: 
Op dit moment heb ik basiskennis van Whole Exome Sequencing en begrijp ik de algemene principes van genetische analyses. Ik heb ervaring met het interpreteren van genetische data, maar ik mis specifieke kennis van het gebruik van GATK-tools voor geavanceerde analyses.

# Volgende Vaardigheden om te leren: 
De volgende vaardigheid die ik moet leren, is het effectief gebruik van GATK-tools voor genetische data-analyse. Dit omvat het begrijpen van de tools, het uitvoeren van variant calling, en het interpreteren van de resultaten voor het identificeren van genetische varianten.

# Leerplan:

## Dag 1 : Data voorbereiden en uitlijnen:
-	Humane referentiegenoom FASTA-sequentie ( GRCH38): er is op de HU server ( /home/daur2/rnaseq/hg38_genome/GRCh38.primary_assembly.genome.fa )
-	GATK-documentatie voor uitlijning kijken en eventuele aanvullende hulpprogramma’s die nodig zijn voor uitlijning downloaden.
-	FASTQ-bestanden voorbereiden, zorg ervoor dat ze van goede kwaliteit zijn.
-	Voer de uitlijning van de reads uit naar de referentiegenoomsequenstie.
-	Indexeer de uitgelijnde BAM-bestanden
-	Begrijp en controleer de kwaliteit van de uitgelijnde gegevens.

## Dag 2: Variant calling en mutatie-identificatie
-	Roep varianten op met GATK HaplotypeCaller of een vergelijkbaar gereedschap.
-	Voer een basisvariantanalyse uit om de opgeroepen varianten te begrijpen.
-	Filter de varianten met basis van kwaliteitscriteria.
-	Identificeer de mutaties door de opgeroepen varianten te analyseren.
-	Documenteer de geïdentificeerde mutatie en genereer een lijst met potentiële interessante varianten.

## Dag 3: Verfijnen en rapporteren
-	Verbeter de workflow en scripts.
-	Voer aanvullende analyse uit om de betrouwbaarheid van de geïdentificeerde mutaties te bevestigen.
-	Werk aan een korte rapport.
-	Zorg ervoor dat al code netjes is georganiseerd.
-	Zorg ervoor een goede documentatie.

#De linken naar de FASTQ bestanden en VCF bestande:
##FASTQ: https://www.internationalgenome.org/data-portal/sample/HG00138
Met Sequence en Exome gekozen.
##VCF: https://www.internationalgenome.org/data-portal/sample/HG00138
Met Variants gekozen   hier staan 20 bestanden bij verschillende chromosomen.

<!--chapter:end:02_Leerplan.Rmd-->

---
title: "Portfolio-opdracht 1.1"
author: "Fatima Danawar"
date: "2023-12-06"
output: html_document
---

# Voorbeeld data analyse ( Analyse van C. elegans Plaatexperiment)
Dit hoofdstuk bevat een gedetailleerde analyse van een C. elegans plaatexperiment. ik zal de gegevens verkennen, visualiseren en analyseren om inzicht te krijgen in de effecten van verschillende verbindingen en concentraties op het aantal nakomelingen.

Om mijn vaardigheden in het werken met basisdatasets te laten zien, heb ik gegevens uit een C. elegans-experiment geïmporteerd en een paar grafieken gemaakt op basis van deze gegevens.
De dataset is aangeleverd door J. Louter (INT/ILC)

Het is een aantal libraries nodig om de data te laden.
```{r, message=FALSE}
# Installeer en laad het readxl-pakket en DT-pakket voor vervolgens analyse
library(here)
library(tidyverse)
library(readxl)
library(DT)
library(ggplot2)
library(dplyr)
```

ik begin met het laden van de gegevens uit het Excel-bestand met behulp van het `readxl`-pakket.
Om data op te laden gebruik ik hier "read_excel()" functie samen met "here()" functie. De read_excel wordt gebruikt om de excel files op te laden in RStudio. 


```{r include=FALSE}
# Lees het Excel-bestand in
c.elegans_data <- readxl::read_excel(here("data/CE.LIQ.FLOW.062_Tidydata.xlsx"))
```

In de onderstaande tabel worden de gegevens weergegeven. Deze datatable wordt gegenereerd met behulp van de "datatable()" functie, waarbij de optie scrollx is ingesteld op true. Hierdoor is het mogelijk om door alle gegevens te scrollen.

```{r}
# de data laden in datatable
datatable(c.elegans_data, options = list(scrollx=TRUE))
```

```{r}
# datatypes en variabelen controleren
str(c.elegans_data)
```

Na het importeren van de gegevens heb ik gecontroleerd welke klassen aan de variabelen waren toegewezen. De meeste variabelen waren correct geïmporteerd. compConcentration werd als een character geïmporteerd, maar het moet een numeric zijn, anders zou de plot er als volgt uitzien:

```{r, warning=FALSE, message=FALSE}
# grafiek met een spreidingsdiagram maken
ggplot(data = c.elegans_data, aes(x = compConcentration, y = RawData)) +
  geom_point(aes(color = compName, 
                 shape = expType),
             size = 1.5, alpha = 0.8)+
  labs(title = "effect van de verbindingsconcentratie op de hoeveelheid levende C. elegans",
       x = "Aantal nakomelingen",
       y = "Concentratie van compound") +
  theme_minimal()
```

Omdat de compConcentration variable als character datatype is, is het niet de juiste plot uitzien.

```{r, warning=FALSE, message=FALSE}
# Verander componCentration naar numeriek
c.elegans_data$compConcentration <- as.numeric(c.elegans_data$compConcentration)

# grafiek maken met Log10 van compConcentrion en het toevoegen van geom_jitter  om de plot leesbaarder te maken
ggplot(data = c.elegans_data, aes(x = log10(compConcentration), y = RawData, color = compName, shape = expType)) +
  geom_jitter(width = 0.2) +
  labs(title = "Effect van de verbindingsconcentratie op de hoeveelheid levende C. elegans",
       x = "Aantal nakomelingen",
       y = "Concentratie van compound") +
  theme_minimal()
```


Vergeleken met de vorige grafiek wordt hier de variabele compConcentration omgezet naar numeriek (en ook de log10 ervan overgenomen). Geom_jitter wordt ook gebruikt, zodat datapunten niet te veel overlappen.

De positieve controle voor dit experiment is Ethanol. De negatieve controle voor dit experiment is S-medium.

oor een statistische analyse om te bepalen of er inderdaad een verschil is, zou ik:
- Shapiro-Wilk normaalheidstest uitvoeren: om te beoordelen of de data een normale verdeling heeft.
- Levene's test uitvoeren: om te onderzoeken of er variabiliteit is binnen de data.
- T-test uitvoeren (indien de data normaal verdeeld is dus de p waarde is groter dan 0,05): om te zien of er een significant verschil is. 

```{r genormaliseerde plot, warning=FALSE, message=FALSE}
# filtr negatieve controle
data_negatieve_controle <- c.elegans_data %>% filter(c.elegans_data$expType == "controlNegative")

# Breken de gemiddelde voor de negative controle
gemiddelde_control <- mean(data_negatieve_controle$RawData, na.rm = TRUE)

# groepen en summerize
summary_celegans <- c.elegans_data %>% group_by(expType, compName, compConcentration) %>% summarize(mean = mean(RawData, na.rm = TRUE))

# Voeg een genormaliseerde kolom toe aan de dataset
data_normalized <- summary_celegans %>%
  mutate(RawData_normalized = mean / gemiddelde_control)

# grafiek maken
ggplot(data = data_normalized, aes(x = log10(compConcentration), y = RawData_normalized)) +
  geom_jitter(aes(color = compName, shape = compName),width = 0.08)+
  labs(title = "Aantal nakomelingen per concentratie (genormaliseerd)",
        x = "log10 Concentratie van compound",
       y = "Genormaliseerd aantal nakomelingen"
      ) +
  theme_minimal()

```

Door te normaliseren tegen de negatieve controle is eenvoudig te zien of een verbinding effect heeft op de hoeveelheid c.elegans. Bovendien is dit nuttig omdat de negatieve controle "normaal" is en dit geen effect zou moeten hebben op de c.elegans. De negatieve controle is dus gelijk aan één.





<!--chapter:end:03_C_elegans_plaatexperiment.Rmd-->

---
title: "Peerreview"
author: "Fatima Danawar"
date: "2023-12-08"
output: html_document
---

# Reproducibility
## Artikel kezien en werken met het
Een van de vaardigheden die nuttig zijn in datascience, is het bekijken van een wetenschappelijk artikel en het gebruik van de code in het artikel om exact dezelfde resultaten te verkrijgen met de dataset. Dit wordt reproduceerbaarheid genoemd.

Een van de vaardigheden die ik heb geleerd, is het beoordelen van een artikel op basis van zijn reproduceerbaarheid. Om mijn vaardigheid op het gebied van het beoordelen van reproduceerbaarheid te tonen, heb ik willekeurig een artikel uit PubMed gehaald. Je kunt dit artikel vinden (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10180718/).

In het artikel "The value of supportive care: A systematic review of cost-effectiveness of non-pharmacological interventions for dementia" heb ik de reproduceerbaarheid beoordeeld aan de hand van specifieke criteria, zoals het doel van de studie, de beschikbaarheid van gegevens, de locatie van de gegevens, en meer. Deze beoordeling stelde me in staat om de mate van transparantie van het artikel vast te stellen en of anderen in staat zouden zijn om mijn resultaten te reproduceren.

### Reference van dit artikel:
Angelica Guzzon, Vincenzo Rebba, Omar Paccagnella, Michela Rigon, Giovanni Boniolo

### Criteria van dit artikel

| Transparency Criteria       | Definition                                                                                                                                                                                                                                                                             | Response Type    |
|-----------------------------|------------------------------------------------------------------------------------------------------------------|------------------|
| Study Purpose               | A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective.                                                                                                      | yes
| Data Availability Statement | A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section.             | yes 
| Data Location               | Where the article’s data can be accessed, either raw or processed.                                               | All relevant data are within the manuscript and its Supporting information files.
| Study Location              | Author has stated in the methods section where the study took place or the data’s country/region of origin.      | Italy
| Author Review               | The professionalism of the contact information that the author has provided in the manuscript.                   | Per mail
| Ethics Statement            | A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data.     | No
| Funding Statement           | A statement within the manuscript indicating whether or not the authors received funding for their research.     | Binary
| Code Availability           | Authors have shared access to the most updated code that they used in their study, including code used for analysis.| yes


### Meer informatie over dit artikel
Algemeen Doel van de Studie

Het algemene doel van de studie is om de waarde van ondersteunende zorg te onderzoeken door middel van een systematische review van de kosteneffectiviteit van niet-farmacologische interventies voor dementie.

Methoden
Voor de methoden van de studie werden systematische literatuuronderzoeken uitgevoerd tussen februari 2019 en december 2021. Verschillende databases werden doorzocht, waaronder PubMed, Cochrane Library, CENTRAL, Embase en PsycINFO. De zoekstrategie was gebaseerd op de PRISMA 2020-aanbevelingen. Studies gepubliceerd tot december 2021 werden overwogen, zonder ondergrens voor de publicatiedatum. Vijf categorieën van ondersteunende zorgstrategieën werden onderscheiden.

Resultaten
Van de 5,479 geïdentificeerde artikelen voldeden 39 aan de inclusiecriteria. Deze studies analyseerden 35 ondersteunende zorgprogramma's op verschillende momenten in het dementiezorgtraject. Elf studies gaven bewijs van hoge kosteneffectiviteit voor zeven interventies, waaronder multicomponent-interventies, indirecte interventies, interventies gericht op zorgverleners en gemeenschapsgerichte programma's.

### Werken met code van artikel (R code paper)

In de wereld van data science is het ook een nuttige vaardigheid om te werken met open science artikelen. Deze artikelen tonen hun code en laten zien hoe ze aan hun gegevens zijn gekomen.
Mijn keuze viel op het artikel 'Infection fatality rate of COVID-19 in community dwelling elderly' in dit link: https://osf.io/w3rzd/.


Deze R-code lijkt te zijn geschreven voor een analyse van de infectie-sterftecijfers (IFR) van COVID-19 in verschillende leeftijdsgroepen, met speciale aandacht voor oudere populaties. Het script verwerkt gegevens uit een Excel-bestand (ifr-data3-younger.xlsx) en voert een reeks bewerkingen en analyses uit.

In termen van leesbaarheid van de code zou ik deze een 3 op 5 geven. Want ik heb errors geregen vanwege niet alle benodigde libraries zijn staan in de code, en het plot zelf is beetje moeilijk om te lezen want daar staan verschillende lijnen die sneden aan elkaar dus de plot niet goed leesbaar.

Als ik zou moeten schalen hoe gemakkelijk de plot is om te reproduceren waarbij 1 heel moeilijk is, en 5 heel gemakkelijk, zou ik het een 5 geven. Dit komt omdat het enige dat veranderd moest worden het pad voor het invoerbestand was.


```{r}
today <- "2021-10-18" #Set today's date in format YYYY-MM-DD

# Load files ----
agebins <- readxl::read_xlsx("C:/Users/Laptop/Downloads/Workflow-Portfolio/data/ifr-data3-younger.xlsx")

# Load packages ----
library(dplyr)
library(ggplot2)
library(RColorBrewer)


### Data management ----

# infected_agebins	----
#Number of infected people in the age bin for date 1.
#Estimated by multiplying the adjusted estimate (and if unavailable, the unadjusted) of
#seroprevalence in the age bin with pop_agebins.

library(rlang)
agebins <- purrr::modify_at(agebins, c("adj_sero_agebins", "crude_sero_agebins", "sero_agebins_g1", "sero_agebins_g2", "sero_agebins_g3", "pop_agebins_g1", "pop_agebins_g2", "pop_agebins_g3", "sero_agebins_g4", "pop_agebins_g4", "pop_agebins", "number_sampled"), ~as.numeric(.x))



library(dplyr)
agebins <- mutate(agebins, infected_agebins = case_when(
  !is.na(adj_sero_agebins) ~ adj_sero_agebins/100*pop_agebins,
  !is.na(sero_agebins_g4) ~
    (sero_agebins_g4/100*pop_agebins_g4 +
    sero_agebins_g3/100*pop_agebins_g3 + 
    sero_agebins_g2/100*pop_agebins_g2 + 
    sero_agebins_g1/100*pop_agebins_g1) /
    (pop_agebins_g4 + pop_agebins_g3 +
     pop_agebins_g2 + pop_agebins_g1) * pop_agebins,
  !is.na(sero_agebins_g3) ~ 
    (sero_agebins_g3/100*pop_agebins_g3 + 
    sero_agebins_g2/100*pop_agebins_g2 + 
    sero_agebins_g1/100*pop_agebins_g1) /
    (pop_agebins_g3 +
       pop_agebins_g2 + pop_agebins_g1) * pop_agebins,
  !is.na(sero_agebins_g2) ~
    (sero_agebins_g2/100*pop_agebins_g2 + 
    sero_agebins_g1/100*pop_agebins_g1) /
    (pop_agebins_g2 + pop_agebins_g1) * pop_agebins,
  !is.na(crude_sero_agebins) ~ crude_sero_agebins/100*pop_agebins,
  TRUE ~ NA_real_
))


# deaths_agebins_date1 ----
#COVID-19 deaths in age bin for date 1, estimated using deaths_agebins_date2, age_date2,
#and deaths_date1
#NAs introduced, that's OK


agebins <- mutate(agebins, deaths_agebins_date1 = case_when(
  age_date2 == "Not applicable" ~ as.numeric(deaths_agebins_date2),
  !is.na(as.numeric(deaths_agebins_date2)) ~ 
    as.numeric(deaths_agebins_date2)/as.numeric(age_date2)*as.numeric(deaths_date1),
  TRUE ~ NA_real_
))

# ifr_uncorr_agebins ----
#IFR in age bins, uncorrected

agebins <- mutate(agebins, ifr_uncorr_agebins = case_when(
  infected_agebins == 0 ~ 0,
  !is.na(deaths_agebins_date1) ~ deaths_agebins_date1/infected_agebins,
  TRUE ~ NA_real_
))


# ifr_corr_agebins ----
#IFR in age bins, corrected

agebins <- mutate(agebins, antibody_type2 = case_when(
  grepl("total|pan.Ig|IgG, IgM, IgA", antibody_type, ignore.case = T) ~ "IgG/IgM/IgA",
  grepl("IgG and.or IgM|IgG, IgM", antibody_type, ignore.case = T) ~ "IgG/IgM",
  grepl("missing|unclear", antibody_type, ignore.case = T) ~ "Missing/Unclear",
  TRUE ~ antibody_type
))

agebins <- mutate(agebins, ifr_corr_agebins = case_when(
  grepl("IgG.IgM.IgA|missing", antibody_type2, ignore.case = T) ~ 
    ifr_uncorr_agebins,
  grepl("^IgG$", antibody_type2, ignore.case = T) ~ ifr_uncorr_agebins*0.9*0.9,
  grepl("IgG.IgM", antibody_type2, ignore.case = T) ~ ifr_uncorr_agebins*0.9
))



# Mid-point of age bin ----
#For Figure
```{r}
agebins <- purrr::modify_at(agebins, c("upper_age", "lower_age"), 
                            ~as.numeric(.x))

agebins$midpoint <- round(agebins$lower_age + 
                            (agebins$upper_age-agebins$lower_age)/2, digits = 1)

#Delete age bins that are not eligible
agebins$width <- agebins$upper_age-agebins$lower_age
agebins <- mutate(agebins, included_plot = case_when(
  grepl("Not include", comment, ignore.case = T) ~ "No",
#  grepl("No", filter_main_analysis, ignore.case = T) ~ "No", #Fix: delete if original set is main set for this analysis
  width < 20 ~ "Yes",
  TRUE ~ "No"
))



agebins <- subset(agebins, included_plot == "Yes")


# Calculation of medians ----

# location2
agebins <- mutate(agebins, location2 = case_when(
  grepl("England|UK|Scotland", location) ~ "UK",
  grepl("Canada", location) ~ "Canada",
  grepl("India", location) ~ "India",
  grepl("Qatar", location) ~ "Qatar",
  grepl("France", location) ~ "France",
  TRUE ~ location
))



#1. Separate datasets per age bin

agebins0_19 <- filter(agebins, midpoint<20)
agebins20_29 <- filter(agebins, midpoint>19.9 & midpoint<30)
agebins30_39 <- filter(agebins, midpoint>29.9 & midpoint<40)
agebins40_49 <- filter(agebins, midpoint>39.9 & midpoint<50)
agebins50_59 <- filter(agebins, midpoint>49.9 & midpoint<60)
agebins60_69 <- filter(agebins, midpoint>59.9 & midpoint<70)
#test <- agebins0_19[c("lower_age", "upper_age", "midpoint")]
#test <- agebins20_29[c("lower_age", "upper_age", "midpoint")]
#test <- agebins30_39[c("lower_age", "upper_age", "midpoint")]
#test <- agebins40_49[c("lower_age", "upper_age", "midpoint")]
#test <- agebins50_59[c("lower_age", "upper_age", "midpoint")]
#test <- agebins60_69[c("lower_age", "upper_age", "midpoint")]



#2. Sample size weighted averages for same country and age bin

agebins0_19 <- group_by(agebins0_19, location2)
n_groups(agebins0_19)
agebins0_19 <- mutate(agebins0_19, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins0_19 <- mutate(agebins0_19, step = ifr_uncorr_agebins*weight)
agebins0_19 <- mutate(agebins0_19, ifr_country_weighted = sum(step))
agebins0_19 <- ungroup(agebins0_19)
agebins0_19$newmidpoint <- "10"
#test <- agebins0_19[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]


agebins20_29 <- group_by(agebins20_29, location2)
n_groups(agebins20_29)
agebins20_29 <- mutate(agebins20_29, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins20_29 <- mutate(agebins20_29, step = ifr_uncorr_agebins*weight)
agebins20_29 <- mutate(agebins20_29, ifr_country_weighted = sum(step))
agebins20_29 <- ungroup(agebins20_29)
agebins20_29$newmidpoint <- "25"
#test <- agebins20_29[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]


agebins30_39 <- group_by(agebins30_39, location2)
n_groups(agebins30_39)
agebins30_39 <- mutate(agebins30_39, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins30_39 <- mutate(agebins30_39, step = ifr_uncorr_agebins*weight)
agebins30_39 <- mutate(agebins30_39, ifr_country_weighted = sum(step))
agebins30_39 <- ungroup(agebins30_39)
agebins30_39$newmidpoint <- "35"
#test <- agebins30_39[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]


agebins40_49 <- group_by(agebins40_49, location2)
n_groups(agebins40_49)
agebins40_49 <- mutate(agebins40_49, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins40_49 <- mutate(agebins40_49, step = ifr_uncorr_agebins*weight)
agebins40_49 <- mutate(agebins40_49, ifr_country_weighted = sum(step))
agebins40_49 <- ungroup(agebins40_49)
agebins40_49$newmidpoint <- "45"
#test <- agebins40_49[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]


agebins50_59 <- group_by(agebins50_59, location2)
n_groups(agebins50_59)
agebins50_59 <- mutate(agebins50_59, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins50_59 <- mutate(agebins50_59, step = ifr_uncorr_agebins*weight)
agebins50_59 <- mutate(agebins50_59, ifr_country_weighted = sum(step))
agebins50_59 <- ungroup(agebins50_59)
agebins50_59$newmidpoint <- "55"
#test <- agebins50_59[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]


agebins60_69 <- group_by(agebins60_69, location2)
n_groups(agebins60_69)
agebins60_69 <- mutate(agebins60_69, weight = case_when(
  !is.na(number_sampled) ~ number_sampled/sum(number_sampled),
  TRUE ~ 1
))
agebins60_69 <- mutate(agebins60_69, step = ifr_uncorr_agebins*weight)
agebins60_69 <- mutate(agebins60_69, ifr_country_weighted = sum(step))
agebins60_69 <- ungroup(agebins60_69)
agebins60_69$newmidpoint <- "65"
#test <- agebins60_69[c("location2", "ifr_uncorr_agebins", "ifr_country_median", "number_sampled", "weight", "ifr_country_weighted")]




#3. Add to main dataset

agebins_new <- rbind(agebins0_19, agebins20_29, agebins30_39, agebins40_49, agebins50_59, agebins60_69)
agebins_new <- group_by(agebins_new, newmidpoint)
agebins_new <- filter(agebins_new, !duplicated(location2))
agebins_new <- mutate(agebins_new, ifr_mdn_weighted = median(ifr_country_weighted))
n_groups(agebins_new)
agebins_new <- ungroup(agebins_new)
test <- select(agebins_new, c("newmidpoint", "ifr_mdn_weighted"))
test$ifr_mdn_weighted <- test$ifr_mdn_weighted*100
test <- subset(test, !duplicated(ifr_mdn_weighted))


### Output ----

# "IFR in younger age-strata" ----

length(unique(agebins$study))
length(unique(agebins$location2))

test #Median IFR in the different age groups (they are here defined by their midpoints)

length(unique(agebins0_19$location2))
length(unique(agebins20_29$location2))
length(unique(agebins30_39$location2))
length(unique(agebins40_49$location2))
length(unique(agebins50_59$location2))
length(unique(agebins60_69$location2))


# Output for the purposes of proofreading ----

writexl::write_xlsx(agebins_new, paste0("ifr-younger-all-variables-", today, ".xlsx"))



# Figure ----

agebins_new <- group_by(agebins_new, newmidpoint)
n_groups(agebins_new)
agebins_new <- filter(agebins_new, !duplicated(location2))
agebins_new <- ungroup(agebins_new)

ageplot1 <- ggplot(agebins_new, aes(x = newmidpoint, y = ifr_country_weighted*100))+
  geom_point(aes(color=location2), size=3)+
  theme_bw()+
  geom_line(aes(group=location2, color=location2))+
  theme(panel.grid = element_blank())+
  labs(y = "IFR (%)", x = "Age", color="Country")+
  scale_color_manual(values = c(brewer.pal(9, "Set1"), 
                                #rev(brewer.pal(8, "Dark2")),
                                rev(brewer.pal(8, "Accent"))))+
  scale_y_log10()+
  theme(strip.background = element_blank())
ageplot1



pdf(paste0("figure-younger-", "today", ".pdf"), height = 5.5, width = 7)

dev.off()
```





<!--chapter:end:04_Peerreview_openen.Rmd-->

---
title: "Relationele databases en SQL"
author: "Fatima Danawar"
date: "2023-12-10"
output: html_document
---

# Relationele databases en SQL
In dit versalg voeg ik drie sets gegevens toe aan een SQL-database en maak ik drie grafieken om mijn vaardigheden in R en SQL te laten zien. Werken met databases zoals SQL is handig voor het beheren van veel gegevens.

Alle benodige libraries laden
```{r, message=FALSE}
library(tidyverse)
library(here)
library(dslabs)
library(grid)
library(readr)
library(RPostgreSQL)
library(gridExtra)
library(ggpubr)
library(stringr)
library(png)
library(DT)
library(ggplot2)
```

In dit chunk heb ik de alle data van github geladen en daarna heb ik alle data wel een tidy gemaakt met behulp met verschillende functies om daarna alle drie data samen met elkaar toe te voegen.
En worden alle data als csv en rds opgeslagd.

```{r}
## Flu data laden

flu_data <- read_csv("https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv", skip = 10)

## Laten we zien de inhoud van de eerste 10 regels van flu data
datatable(flu_data, options = list(scrollx=TRUE, pageLength = 10))

## Dengue data laden
dengue_data <- read_csv("https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv", skip = 10)

## Laten we zien de inhoud van de eerste 10 regels van dengue data
datatable(dengue_data, options = list(scrollx=TRUE, pageLength = 10))

## Gapminder data van de dslabs package
gapminder_data <- gapminder

## Laten we zien de inhoud van de eerste 10 regels van gapminder data
datatable(gapminder_data, options = list(scrollx=TRUE, pageLength = 10))

#Data Tidy maken
##Flu data tidy 
flu_data_tidy <- flu_data %>% pivot_longer(cols = -Date, names_to = "country", values_to = "flu_aantal")

## Verwijder de dag en de maand van de Date kolom, hernomen van de Date kolom naar Year en aanpassen van variabelen data typen 
flu_data_tidy$Date <- str_sub(flu_data_tidy$Date, start = 1, end = 4)
flu_data_tidy <- rename(flu_data_tidy, year = Date)
flu_data_tidy$country <- as.factor(flu_data_tidy$country)
flu_data_tidy$year <- as.integer(flu_data_tidy$year)

## Laten we zien hoe ziet de data nu eruit
datatable(flu_data_tidy, options = list(scrollx=TRUE, pageLength = 10))

## Dengue data tidy 
dengue_data_tidy <- dengue_data %>% pivot_longer(cols = -Date, names_to = "country", values_to = "dengue_aantal")

## Verwijder de dag en de maand van de Date kolom, hernomen van de Date kolom naar Year en aanpassen van variabelen data typen
dengue_data_tidy$Date <- str_sub(dengue_data_tidy$Date, start = 1, end = 4)
dengue_data_tidy <- rename(dengue_data_tidy, year = Date)
dengue_data_tidy$year <- as.integer(dengue_data_tidy$year)
dengue_data_tidy$country <- as.factor(dengue_data_tidy$country)

##Laten we zien hoe ziet de data nu eruit
datatable(dengue_data_tidy, options = list(scrollx=TRUE, pageLength = 10))

# Opslaan alle drie dataframes als csv en rds bestanden
## Als csv bestanden
write.csv(flu_data_tidy, file = here("data/flu.csv"), row.names = FALSE)
write.csv(dengue_data_tidy, file = here("data/dengue.csv"), row.names = FALSE)
write.csv(gapminder_data, file = here("data/gapminder_data.csv"), row.names = FALSE)

## Als rds bestanden
saveRDS(flu_data_tidy, file = here("data/flu_data.rds"))
saveRDS(dengue_data_tidy, file = here("data/dengue_data.rds"))
saveRDS(gapminder_data, file = here("data/gapminder_data.rds"))
```

De volgende stap is het maken van nieuwe PostgreSQL database op DBeaver met dit code : create database workflowsdb.


## Maak verbinding met de database
con <- dbConnect(RPostgres::Postgres(),
                                 dbname = "workflowsdb",
                                 host = "localhost",
                                 port = "5432",
                                 user = "postgres",
                                 password = "Fatima1996@")
### SQL-scripts voor het schrijven van tabellen naar de database vanuit dataframes

dbWriteTable(con, "flu_data_table", flu_data_tidy, overwrite = TRUE)
dbWriteTable(con, "dengue_data_table", dengue_data_tidy, overwrite = TRUE)
dbWriteTable(con, "gapminder_data_table", gapminder_data, overwrite = TRUE)

Disconnect van de database
dbDisconnect(con)

inhoud bekijken van flu_data_table
select 
*
from 
flu_data_table 

inhoud bekijken van dengue_data_table
select 
*
from 
dengue_data_table  

inhoud bekijken van gapminder_data_table
select 
*
from 
gapminder_data_table


Bij onderstande figuur, het is de opgeslaagde SQL script
```{r figuur van SQL script}
sql_script <- rasterGrob(as.raster(readPNG(here("Images/saved_sqlscript.png"))))

# laat de figuur zien
grid.arrange(sql_script)
```

## Samenvoegende table van alle drie data
De sql script van het samenvogen table van alle drie data is in onderstaande figuur
```{r sql script van het samenvoegen van alle drie data tot een table}
samenvogende_sqlscript <- rasterGrob(as.raster(readPNG(here("Images/samenvoegende_table.png"))))
grid.arrange(samenvogende_sqlscript)
```

Het laden van de samengevoegde table 
```{r ladan van de samengevoegde table}
samengevoegde_table_data <- read.csv(here("data/samenvoegende_table_202312111108.csv"))
## laten we de inhoud zien
datatable(samengevoegde_table_data, options = list(scrollx=TRUE, pageLength = 10))
```

## Drie statistieken met samenvogende table
In dit project heb ik gegevens samengevoegd van verschillende tabellen en beschrijvende statistieken gegenereerd. Hieronder vindt u een overzicht van mijn aanpak en de resultaten.
```{r}
## Symmary statistieken
summary(samengevoegde_table_data)

## Visualisatie 1: 
```

<!--chapter:end:05_SQL.Rmd-->

# Data managment met Gruerrilla analytics structure
 In dit opdracht heb ik een tree gemaakt voor DAUR2 mapje met behulp van Gruerrilla analytics structure
 
```{r}
fs::dir_tree(here::here("DAUR2"))
```

<!--chapter:end:06_Gruerrilla_tree_DAUR2.Rmd-->


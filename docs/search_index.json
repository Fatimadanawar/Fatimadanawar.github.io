[["index.html", "bibliography 1 Portfolio Inleiding 1.1 Over mij", " bibliography Fatima Danawar 2023-12-19 1 Portfolio Inleiding Welkom bij mijn portfolio! In dit document deel ik een overzicht van mijn datascience-vaardigheden en -projecten. Als aspirant-datascientist heb ik mijn reis vastgelegd door middel van hands-on projecten en praktische ervaringen, met de nadruk op het gebruik van R, SQL en gerelateerde tools. 1.1 Over mij Mijn naam is Fatima , een enthousiaste datascience-enthousiasteling met een passie voor het begrijpen en analyseren van gegevens om waardevolle inzichten te verkrijgen. Tijdens mijn leertraject heb ik me gericht op het ontwikkelen van vaardigheden op het gebied van gegevensverwerking, analyse en visualisatie. "],["curriculum-vitae.html", "2 Curriculum Vitae 2.1 Mijn CV met vitae R package 2.2 Script", " 2 Curriculum Vitae 2.1 Mijn CV met vitae R package 2.2 Script The script I have used for creating this CV can be seen here on my github. "],["mijn-toekomst-en-gatk-tools.html", "3 Mijn toekomst en GATK tools 3.1 Doel: 3.2 Huidige Status: 3.3 Volgende Vaardigheden om te leren: 3.4 Leerplan: 3.5 Voorbereiding 3.6 Resultaten 3.7 Evaluatie", " 3 Mijn toekomst en GATK tools 3.1 Doel: Over ~2 jaar wil ik een ervaren professional zijn in het interpreteren van Whole Exome Sequencing (WES) met een specialisatie in het identificeren van genetische oorzaken van zeldzame genetische aandoeningen, met name het KBG-syndroom. Ik wil diepere expertise ontwikkelen in het gebruik van geavanceerde tools zoals GATK om nauwkeurige genetische analyses uit te voeren. 3.2 Huidige Status: Op dit moment heb ik basiskennis van Whole Exome Sequencing en begrijp ik de algemene principes van genetische analyses. Ik heb ervaring met het interpreteren van genetische data, maar ik mis specifieke kennis van het gebruik van GATK-tools voor geavanceerde analyses. 3.3 Volgende Vaardigheden om te leren: De volgende vaardigheid die ik moet leren, is het effectief gebruik van GATK-tools voor genetische data-analyse. Dit omvat het begrijpen van de tools, het uitvoeren van variant calling, en het interpreteren van de resultaten voor het identificeren van genetische varianten. 3.4 Leerplan: 3.4.1 Dag 1 : Project Initiatie &amp; Tool Setup Referentie materiaal lezen en bestuderen: Voordat er wordt begonnen, zal het referentiemateriaal worden gelezen en bestudeerd. Dit omvat documentatie, handleidingen en relevante literatuur over variant calling en bioinformatica-analyse. Data downloaden: De ruwe sequencingdata die nodig zijn voor de analyse zal worden gedownload gebruik van 1000 genoomes Nodige programma’s installeren: Om de analyse uit te voeren, zullen verschillende bioinformatica-tools en software worden geïnstalleerd. Dit omvat programma’s zoals GATK, BWA, samtools en eventuele andere benodigde tools. 3.4.2 Dag 2: Data voorbereiden voor analyse Verwerken van ruwe sequencing data: De ruwe sequencingdata zal worden verwerkt om ze klaar te maken voor variant calling. Dit omvat stappen zoals kwaliteitscontrole, trimmen van adapters en kwaliteitsfiltering. Voorbereiden van referentiegenoom: Het referentiegenoom zal worden voorbereid voor gebruik in de analyse. Dit omvat het downloaden, indexeren en eventueel annoteren van het referentiegenoom. 3.4.3 Dag 3: Variant Calling Pipeline Variant calling uitvoeren: Variant calling zal worden uitgevoerd op de verwerkte sequencingdata met behulp van tools zoals GATK. Dit omvat het identificeren van variaties ten opzichte van het referentiegenoom. Annotatie van Varianten bestuderen: Na het oproepen van varianten zullen de varianten worden geannoteerd en bestudeerd. Dit omvat het identificeren van genetische varianten en het begrijpen van hun mogelijke impact op biologische processen. 3.5 Voorbereiding Stap 1: Installeren van Benodigde Software De installatie van de benodigde softwarepakketten en tools werd gestart. Een conda-omgeving werd gebruikt om de dependencies te beheren. # Maak een nieuwe conda omgeving aan en activeer deze conda create -n variant_calling conda activate variant_calling # Installeer Samtools, BWA, FastQC en GATK conda install -c bioconda samtools bwa fastqc conda install -c conda-forge openjdk=17 Stap 2: Data Downloaden en Voorbereiden Daarna werd de benodigde data gedownload, waaronder het referentiegenoom en de sequencing reads. Deze data werd opgeslagen in gestructureerde mappen. # Maak benodigde directories aan mkdir -p ~/variant_calling/{aligned_reads,reads,scripts,results,tools,referentie_genoom} # Download sequencing reads wget -P ~/variant_calling/reads ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR062634_1.filt.fastq.gz wget -P ~/variant_calling/reads ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/phase3/data/HG00096/sequence_read/SRR062634_2.filt.fastq.gz # GATK downloden en installeren wget -P ~/variant_calling/tools https://github.com/broadinstitute/gatk/releases/download/4.5.0.0/gatk-4.5.0.0.zip unzip ~/variant_calling/tools/gatk-4.5.0.0.zip # Download en prepareer de referentiegenoom wget -P ~/variant_calling/referentie_genoom/ https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz gunzip ~/variant_calling/referentie_genoom/hg38.fa.gz samtools faidx ~/variant_calling/referentie_genoom/hg38.fa java -jar ~/variant_calling/tools/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar CreateSequenceDictionary R=~/variant_calling/referentie_genoom/hg38.fa O=~/variant_calling/referentie_genoom/hg38.dict # Download know sites files for BQSR from GATK resource bundle wget -P ~/variant_calling/referentie_genoom/ https://storage.googleapis.com/genomics-public-data/references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf wget -P ~/variant_calling/referentie_genoom/ https://storage.googleapis.com/genomics-public-data/references/hg38/v0/Homo_sapiens_assembly38.dbsnp138.vcf.idx Stap 3: Kwaliteitscontrole en Mapping Na het downloaden van de data werden de sequencing reads gecontroleerd op kwaliteit met FastQC. Vervolgens werden ze gemapt naar het referentiegenoom met behulp van BWA. Dit proces zorgde voor een nauwkeurige uitlijning van de reads op de juiste locaties in het genoom, wat essentieel is voor verdere variant calling en mutatie-identificatie. # Run FastQC voor kwaliteitscontrole fastqc ~/variant_calling/reads/SRR062634_1.filt.fastq.gz -o ~/variant_calling/reads/ fastqc ~/variant_calling/reads/SRR062634_2.filt.fastq.gz -o ~/variant_calling/reads/ # Map reads naar referentiegenoom met BWA bwa index ~/variant_calling/referentie_genoom/hg38.fa bwa mem -t 4 -R &quot;@RG\\tID:SRR062634\\tPL:ILLUMINA\\tSM:SRR062634&quot; ~/variant_calling/referentie_genoom/hg38.fa ~/variant_calling/reads/SRR062634_1.filt.fastq.gz ~/variant_calling/reads/SRR062634_2.filt.fastq.gz &gt; ~/variant_calling/aligned_reads/SRR062634.paired.sam Stap 4: Markeren van Duplicaten en Basiskwaliteit Herkalibratie Duplicaten werden gemarkeerd en de basiskwaliteit werd herkalibreerd om de nauwkeurigheid van de variant calling te verbeteren. Deze stap is cruciaal om ervoor te zorgen dat eventuele artefacten in de data worden geïdentificeerd en gecorrigeerd, waardoor betrouwbare resultaten worden gegarandeerd. # Markeer duplicaten en sorteer de reads java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar MarkDuplicatesSpark -I ${aligned_reads}/SRR062634.paired.sam -O ${aligned_reads}/SRR062634_sorted_dedup_reads.bam # Basiskwaliteit herkalibratie # 1.build the model java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar BaseRecalibrator -I ${aligned_reads}/SRR062634_sorted_dedup_reads.bam -R ${ref} --known-sites ${know_sites} -O ${data}/recal_data.table # 2.Apply the model to adjust the base quality scores java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar ApplyBQSR -I ${aligned_reads}/SRR062634_sorted_dedup_reads.bam -R ${ref} --bqsr-recal-file ${data}/recal_data.table -O ${aligned_reads}/SRR062634_sorted_dedup_bqsr_reads.bam # Collect Alignment &amp; insert Size Metrics java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar CollectAlignmentSummaryMetrics R=${ref} I=${aligned_reads}/SRR062634_sorted_dedup_bqsr_reads.bam O=${aligned_reads}/alignment_metrics.txt java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar CollectInsertSizeMetrics INPUT=${aligned_reads}/SRR062634_sorted_dedup_bqsr_reads.bam OUTPUT=${aligned_reads}/insert_size_metrics.txt HISTOGRAM_FILE=${aligned_reads}/insert_size_histogram.pdf Stap 5: Variant Calling en Filtering De variant calling werd uitgevoerd met GATK HaplotypeCaller, gevolgd door het filteren en annoteren van de gevonden varianten. Deze fase vereist zorgvuldige aandacht voor detail om ervoor te zorgen dat alleen betrouwbare varianten worden behouden en dat eventuele artefacten worden geëlimineerd. # Voer variant calling uit met HaplotypeCaller java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar HaplotypeCaller -R ${ref} -I ${aligned_reads}/SRR062634_sorted_dedup_bqsr_reads.bam -O ${results}/raw_variantcs.vcf ## extract SNPs &amp; INDELS java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar SelectVariants -R ${ref} -V ${results}/raw_variantcs.vcf --select-type SNP -O ${results}/raw_snps.vcf java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar SelectVariants -R ${ref} -V ${results}/raw_variantcs.vcf --select-type INDEL -O ${results}/raw_indels.vcf # Filter varianten ## Filter SNPS java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar VariantFiltration \\ -R ${ref} \\ -V ${results}/raw_snps.vcf \\ -O ${results}/filtered_snps.vcf \\ -filter-name &quot;QD_filter&quot; -filter &quot;QD &lt; 2.0&quot; \\ -filter-name &quot;FS_filter&quot; -filter &quot;FS &gt; 60.0&quot; \\ -filter-name &quot;MQ_filter&quot; -filter &quot;MQ &lt; 40.0&quot; \\ -filter-name &quot;SOR_filter&quot; -filter &quot;SOR &gt; 4.0&quot; \\ -filter-name &quot;MQRankSum_filter&quot; -filter &quot;MQRankSum &lt; -12.5&quot; \\ -filter-name &quot;ReadPosRankSum_filter&quot; -filter &quot;ReadPosRankSum &lt; -8.0&quot; \\ -genotype-filter-expression &quot;DP &lt; 10&quot; \\ -genotype-filter-name &quot;DP_filter&quot; \\ -genotype-filter-expression &quot;GQ &lt; 10&quot; \\ -genotype-filter-name &quot;GQ_filter&quot; ## Filter INDELS java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar VariantFiltration \\ -R ${ref} \\ -V ${results}/raw_indels.vcf \\ -O ${results}/filtered_indels.vcf \\ -filter-name &quot;QD_filter&quot; -filter &quot;QD &lt; 2.0&quot; \\ -filter-name &quot;FS_filter&quot; -filter &quot;FS &gt; 200.0&quot; \\ -filter-name &quot;MQ_filter&quot; -filter &quot;MQ &lt; 40.0&quot; \\ -filter-name &quot;SOR_filter&quot; -filter &quot;SOR &gt; 10.0&quot; \\ -genotype-filter-expression &quot;DP &lt; 10&quot; \\ -genotype-filter-name &quot;DP_filter&quot; \\ -genotype-filter-expression &quot;GQ &lt; 10&quot; \\ -genotype-filter-name &quot;GQ_filter&quot; ## Select Variants that PASS filters java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar SelectVariants \\ --exclude-filtered \\ -V ${results}/filtered_snps.vcf \\ -O ${results}/analysis-ready-snps.vcf java -jar ${tools}/gatk-4.5.0.0/gatk-package-4.5.0.0-local.jar SelectVariants \\ --exclude-filtered \\ -V ${results}/filtered_indels.vcf \\ -O ${results}/analysis-ready-indels.vcf ## To exclude variants that failed genotype filters cat ${results}/analysis-ready-snps.vcf | grep -v -E &quot;DP_filter|GQ_filter&quot; &gt; ${results}/analysis-ready-snps-filteredGT.vcf cat ${results}/analysis-ready-indels.vcf | grep -v -E &quot;DP_filter|GQ_filter&quot; &gt; ${results}/analysis-ready-indels-filteredGT.vcf 3.6 Resultaten Kwaliteitscontrole en Mapping De kwaliteit van de sequencing reads werd beoordeeld en bleek acceptabel te zijn, wat het vermogen om tot nauwkeurige mapping te komen, ondersteunde. Gedurende het mappingproces met BWA deden zich geen noemenswaardige problemen voor, wat resulteerde in een succesvolle toewijzing van de reads naar het referentiegenoom. Basiskwaliteit Herkalibratie Na de herkalibratie van de basiskwaliteit werd een merkbare verbetering waargenomen in de nauwkeurigheid van de reads. Deze stap was cruciaal om de betrouwbaarheid van de uiteindelijke variant calling te versterken, waardoor de nauwkeurigheid van de analyse toenam. Variant Calling en Filtering Het uitvoeren van variant calling met GATK HaplotypeCaller genereerde een aanzienlijk aantal ruwe varianten. Deze werden vervolgens onderworpen aan strikte filtering op basis van verschillende kwaliteitscriteria. Dit proces resulteerde in een verfijnde lijst van varianten die weliswaar kleiner was, maar een hogere mate van betrouwbaarheid bood voor verdere analyse. Annotatie van Varianten Na de filtering werden de overgebleven varianten geannoteerd met SnpEff. Deze annotatie voorzag de varianten van aanvullende biologische context, wat essentieel was voor een diepgaande interpretatie van de bevindingen. java -jar ${tools}/snpEff/snpEff.jar hg38 ${results}/analysis-ready-snps-filteredGT.vcf &gt; ${results}/analysis-ready-snps-filteredGT-annotated.vcf java -jar ${tools}/snpEff/snpEff.jar hg38 ${results}/analysis-ready-indels-filteredGT.vcf &gt; ${results}/analysis-ready-indels-filteredGT-annotated.vcf De hele script variant_calling.sh van deze opdrchat kun je vinden in variant_calling script en hier kun je de data managment tree van de results folder results_folder_tree &lt;- rasterGrob(as.raster(readPNG(here(&quot;GATK_tools/results_folder_tree.png&quot;)))) grid.arrange(results_folder_tree) Hier kun je de snps varianten zien en op welke regeio en zijn effect voor meer informatie kun je zien in deze file snpEff_snps_summary.html snpEff snps summary varianten_snps &lt;- rasterGrob(as.raster(readPNG(here(&quot;GATK_tools/varianten_by_snps.png&quot;)))) grid.arrange(varianten_snps) regio_effect_snps &lt;- rasterGrob(as.raster(readPNG(here(&quot;GATK_tools/region_effect_snps.png&quot;)))) grid.arrange(regio_effect_snps) Hier kun je de snps varianten zien en op welke regeio en zijn effect voor meer informatie kun je zien in deze file snpEff_indels_summary.html snpEff endels summary varianten_indels &lt;- rasterGrob(as.raster(readPNG(here(&quot;GATK_tools/varianten_by_indels.png&quot;)))) grid.arrange(varianten_indels) regio_effect_indels &lt;- rasterGrob(as.raster(readPNG(here(&quot;GATK_tools/region_effect_indels.png&quot;)))) grid.arrange(regio_effect_indels) # de snpEff_genes_snps data laden in datatable datatable(snpEff_genes_snps, options = list(scrollx=TRUE)) # de snpEff_genes_indels data laden in datatable datatable(snpEff_genes_indels, options = list(scrollx=TRUE)) 3.7 Evaluatie Dit project bood een uiterst leerzame ervaring en gaf mij de gelegenheid om praktijkervaring op te doen met variant calling en de bijbehorende tools. Ondanks de beperkte tijd ben ik tevreden met de behaalde resultaten en heb ik veel geleerd over de verschillende stappen en uitdagingen in het proces van variant calling. Deze hands-on ervaring heeft mijn begrip van bioinformatica aanzienlijk verbeterd en heeft mij waardevolle inzichten gegeven in de complexiteit van genoomanalyse. Ik kijk ernaar uit om deze verworven kennis verder uit te breiden en toe te passen in toekomstige projecten, waarbij ik mijn vaardigheden verder zal ontwikkelen en mijn bijdrage aan het vakgebied zal vergroten. "],["voorbeeld-data-analyse-analyse-van-c.-elegans-plaatexperiment.html", "4 Voorbeeld data analyse ( Analyse van C. elegans Plaatexperiment)", " 4 Voorbeeld data analyse ( Analyse van C. elegans Plaatexperiment) Dit hoofdstuk bevat een gedetailleerde analyse van een C. elegans plaatexperiment. ik zal de gegevens verkennen, visualiseren en analyseren om inzicht te krijgen in de effecten van verschillende verbindingen en concentraties op het aantal nakomelingen. Om mijn vaardigheden in het werken met basisdatasets te laten zien, heb ik gegevens uit een C. elegans-experiment geïmporteerd en een paar grafieken gemaakt op basis van deze gegevens. De dataset is aangeleverd door J. Louter (INT/ILC) Het is een aantal libraries nodig om de data te laden. # Installeer en laad het readxl-pakket en DT-pakket voor vervolgens analyse library(here) library(tidyverse) library(readxl) library(DT) library(ggplot2) library(dplyr) ik begin met het laden van de gegevens uit het Excel-bestand met behulp van het readxl-pakket. Om data op te laden gebruik ik hier “read_excel()” functie samen met “here()” functie. De read_excel wordt gebruikt om de excel files op te laden in RStudio. In de onderstaande tabel worden de gegevens weergegeven. Deze datatable wordt gegenereerd met behulp van de “datatable()” functie, waarbij de optie scrollx is ingesteld op true. Hierdoor is het mogelijk om door alle gegevens te scrollen. # de data laden in datatable datatable(c.elegans_data, options = list(scrollx=TRUE)) # datatypes en variabelen controleren str(c.elegans_data) ## tibble [360 × 34] (S3: tbl_df/tbl/data.frame) ## $ plateRow : logi [1:360] NA NA NA NA NA NA ... ## $ plateColumn : logi [1:360] NA NA NA NA NA NA ... ## $ vialNr : num [1:360] 1 1 1 1 1 2 2 2 2 2 ... ## $ dropCode : chr [1:360] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;d&quot; ... ## $ expType : chr [1:360] &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; &quot;experiment&quot; ... ## $ expReplicate : num [1:360] 3 3 3 3 3 3 3 3 3 3 ... ## $ expName : chr [1:360] &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; &quot;CE.LIQ.FLOW.062&quot; ... ## $ expDate : POSIXct[1:360], format: &quot;2020-11-30&quot; &quot;2020-11-30&quot; ... ## $ expResearcher : chr [1:360] &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; &quot;Sergio Reijnders - Ellis Herder&quot; ... ## $ expTime : num [1:360] 68 68 68 68 68 68 68 68 68 68 ... ## $ expUnit : chr [1:360] &quot;hour&quot; &quot;hour&quot; &quot;hour&quot; &quot;hour&quot; ... ## $ expVolumeCounted : num [1:360] 50 50 50 50 50 50 50 50 50 50 ... ## $ RawData : num [1:360] 44 37 45 47 41 35 41 36 40 38 ... ## $ compCASRN : chr [1:360] &quot;24157-81-1&quot; &quot;24157-81-1&quot; &quot;24157-81-1&quot; &quot;24157-81-1&quot; ... ## $ compName : chr [1:360] &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; &quot;2,6-diisopropylnaphthalene&quot; ... ## $ compConcentration : chr [1:360] &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; &quot;4.99&quot; ... ## $ compUnit : chr [1:360] &quot;nM&quot; &quot;nM&quot; &quot;nM&quot; &quot;nM&quot; ... ## $ compDelivery : chr [1:360] &quot;Liquid&quot; &quot;Liquid&quot; &quot;Liquid&quot; &quot;Liquid&quot; ... ## $ compVehicle : chr [1:360] &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; &quot;controlVehicleA&quot; ... ## $ elegansStrain : chr [1:360] &quot;N2&quot; &quot;N2&quot; &quot;N2&quot; &quot;N2&quot; ... ## $ elegansInput : num [1:360] 25 25 25 25 25 25 25 25 25 25 ... ## $ bacterialStrain : chr [1:360] &quot;OP50&quot; &quot;OP50&quot; &quot;OP50&quot; &quot;OP50&quot; ... ## $ bacterialTreatment : chr [1:360] &quot;heated&quot; &quot;heated&quot; &quot;heated&quot; &quot;heated&quot; ... ## $ bacterialOD600 : num [1:360] 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 0.743 ... ## $ bacterialConcX : num [1:360] 8 8 8 8 8 8 8 8 8 8 ... ## $ bacterialVolume : num [1:360] 300 300 300 300 300 300 300 300 300 300 ... ## $ bacterialVolUnit : chr [1:360] &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; ... ## $ incubationVial : chr [1:360] &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; &quot;1,5 glass vial&quot; ... ## $ incubationVolume : num [1:360] 1000 1000 1000 1000 1000 1000 1000 1000 1000 1000 ... ## $ incubationUnit : chr [1:360] &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; &quot;ul&quot; ... ## $ incubationMethod : chr [1:360] &quot;rockroll&quot; &quot;rockroll&quot; &quot;rockroll&quot; &quot;rockroll&quot; ... ## $ incubationRPM : num [1:360] 35 35 35 35 35 35 35 35 35 35 ... ## $ bubble : logi [1:360] NA NA NA NA NA NA ... ## $ incubateTemperature: num [1:360] 20 20 20 20 20 20 20 20 20 20 ... Na het importeren van de gegevens heb ik gecontroleerd welke klassen aan de variabelen waren toegewezen. De meeste variabelen waren correct geïmporteerd. compConcentration werd als een character geïmporteerd, maar het moet een numeric zijn, anders zou de plot er als volgt uitzien: # grafiek met een spreidingsdiagram maken ggplot(data = c.elegans_data, aes(x = compConcentration, y = RawData)) + geom_point(aes(color = compName, shape = expType), size = 1.5, alpha = 0.8)+ labs(title = &quot;effect van de verbindingsconcentratie op de hoeveelheid levende C. elegans&quot;, x = &quot;Aantal nakomelingen&quot;, y = &quot;Concentratie van compound&quot;) + theme_minimal() Omdat de compConcentration variable als character datatype is, is het niet de juiste plot uitzien. # Verander componCentration naar numeriek c.elegans_data$compConcentration &lt;- as.numeric(c.elegans_data$compConcentration) # grafiek maken met Log10 van compConcentrion en het toevoegen van geom_jitter om de plot leesbaarder te maken ggplot(data = c.elegans_data, aes(x = log10(compConcentration), y = RawData, color = compName, shape = expType)) + geom_jitter(width = 0.2) + labs(title = &quot;Effect van de verbindingsconcentratie op de hoeveelheid levende C. elegans&quot;, x = &quot;Aantal nakomelingen&quot;, y = &quot;Concentratie van compound&quot;) + theme_minimal() Vergeleken met de vorige grafiek wordt hier de variabele compConcentration omgezet naar numeriek (en ook de log10 ervan overgenomen). Geom_jitter wordt ook gebruikt, zodat datapunten niet te veel overlappen. De positieve controle voor dit experiment is Ethanol. De negatieve controle voor dit experiment is S-medium. oor een statistische analyse om te bepalen of er inderdaad een verschil is, zou ik: - Shapiro-Wilk normaalheidstest uitvoeren: om te beoordelen of de data een normale verdeling heeft. - Levene’s test uitvoeren: om te onderzoeken of er variabiliteit is binnen de data. - T-test uitvoeren (indien de data normaal verdeeld is dus de p waarde is groter dan 0,05): om te zien of er een significant verschil is. # filtr negatieve controle data_negatieve_controle &lt;- c.elegans_data %&gt;% filter(c.elegans_data$expType == &quot;controlNegative&quot;) # Breken de gemiddelde voor de negative controle gemiddelde_control &lt;- mean(data_negatieve_controle$RawData, na.rm = TRUE) # groepen en summerize summary_celegans &lt;- c.elegans_data %&gt;% group_by(expType, compName, compConcentration) %&gt;% summarize(mean = mean(RawData, na.rm = TRUE)) # Voeg een genormaliseerde kolom toe aan de dataset data_normalized &lt;- summary_celegans %&gt;% mutate(RawData_normalized = mean / gemiddelde_control) # grafiek maken ggplot(data = data_normalized, aes(x = log10(compConcentration), y = RawData_normalized)) + geom_jitter(aes(color = compName, shape = compName),width = 0.08)+ labs(title = &quot;Aantal nakomelingen per concentratie (genormaliseerd)&quot;, x = &quot;log10 Concentratie van compound&quot;, y = &quot;Genormaliseerd aantal nakomelingen&quot; ) + theme_minimal() Door te normaliseren tegen de negatieve controle is eenvoudig te zien of een verbinding effect heeft op de hoeveelheid c.elegans. Bovendien is dit nuttig omdat de negatieve controle “normaal” is en dit geen effect zou moeten hebben op de c.elegans. De negatieve controle is dus gelijk aan één. "],["reproducibility.html", "5 Reproducibility 5.1 Artikel kezien en werken met het", " 5 Reproducibility 5.1 Artikel kezien en werken met het Een van de vaardigheden die nuttig zijn in datascience, is het bekijken van een wetenschappelijk artikel en het gebruik van de code in het artikel om exact dezelfde resultaten te verkrijgen met de dataset. Dit wordt reproduceerbaarheid genoemd. Een van de vaardigheden die ik heb geleerd, is het beoordelen van een artikel op basis van zijn reproduceerbaarheid. Om mijn vaardigheid op het gebied van het beoordelen van reproduceerbaarheid te tonen, heb ik willekeurig een artikel uit PubMed gehaald. Je kunt dit artikel vinden (https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10180718/). In het artikel “The value of supportive care: A systematic review of cost-effectiveness of non-pharmacological interventions for dementia” heb ik de reproduceerbaarheid beoordeeld aan de hand van specifieke criteria, zoals het doel van de studie, de beschikbaarheid van gegevens, de locatie van de gegevens, en meer. Deze beoordeling stelde me in staat om de mate van transparantie van het artikel vast te stellen en of anderen in staat zouden zijn om mijn resultaten te reproduceren. 5.1.1 Reference van dit artikel: Angelica Guzzon, Vincenzo Rebba, Omar Paccagnella, Michela Rigon, Giovanni Boniolo 5.1.2 Criteria van dit artikel Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. yes Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a study’s data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. yes Data Location Where the article’s data can be accessed, either raw or processed. All relevant data are within the manuscript and its Supporting information files. Study Location Author has stated in the methods section where the study took place or the data’s country/region of origin. Italy Author Review The professionalism of the contact information that the author has provided in the manuscript. Per mail Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. No Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. yes 5.1.3 Meer informatie over dit artikel Algemeen Doel van de Studie Het algemene doel van de studie is om de waarde van ondersteunende zorg te onderzoeken door middel van een systematische review van de kosteneffectiviteit van niet-farmacologische interventies voor dementie. Methoden Voor de methoden van de studie werden systematische literatuuronderzoeken uitgevoerd tussen februari 2019 en december 2021. Verschillende databases werden doorzocht, waaronder PubMed, Cochrane Library, CENTRAL, Embase en PsycINFO. De zoekstrategie was gebaseerd op de PRISMA 2020-aanbevelingen. Studies gepubliceerd tot december 2021 werden overwogen, zonder ondergrens voor de publicatiedatum. Vijf categorieën van ondersteunende zorgstrategieën werden onderscheiden. Resultaten Van de 5,479 geïdentificeerde artikelen voldeden 39 aan de inclusiecriteria. Deze studies analyseerden 35 ondersteunende zorgprogramma’s op verschillende momenten in het dementiezorgtraject. Elf studies gaven bewijs van hoge kosteneffectiviteit voor zeven interventies, waaronder multicomponent-interventies, indirecte interventies, interventies gericht op zorgverleners en gemeenschapsgerichte programma’s. 5.1.4 Werken met code van artikel (R code paper) In de wereld van data science is het ook een nuttige vaardigheid om te werken met open science artikelen. Deze artikelen tonen hun code en laten zien hoe ze aan hun gegevens zijn gekomen. Mijn keuze viel op het artikel ‘Infection fatality rate of COVID-19 in community dwelling elderly’ in dit link: https://osf.io/w3rzd/. Deze R-code lijkt te zijn geschreven voor een analyse van de infectie-sterftecijfers (IFR) van COVID-19 in verschillende leeftijdsgroepen, met speciale aandacht voor oudere populaties. Het script verwerkt gegevens uit een Excel-bestand (ifr-data3-younger.xlsx) en voert een reeks bewerkingen en analyses uit. In termen van leesbaarheid van de code zou ik deze een 3 op 5 geven. Want ik heb errors geregen vanwege niet alle benodigde libraries zijn staan in de code, en het plot zelf is beetje moeilijk om te lezen want daar staan verschillende lijnen die sneden aan elkaar dus de plot niet goed leesbaar. Als ik zou moeten schalen hoe gemakkelijk de plot is om te reproduceren waarbij 1 heel moeilijk is, en 5 heel gemakkelijk, zou ik het een 5 geven. Dit komt omdat het enige dat veranderd moest worden het pad voor het invoerbestand was. "],["relationele-databases-en-sql.html", "6 Relationele databases en SQL 6.1 Maak verbinding met de database 6.2 Samenvoegende table van alle drie data 6.3 Drie statistieken met samenvogende table", " 6 Relationele databases en SQL In dit versalg voeg ik drie sets gegevens toe aan een SQL-database en maak ik drie grafieken om mijn vaardigheden in R en SQL te laten zien. Werken met databases zoals SQL is handig voor het beheren van veel gegevens. Alle benodige libraries laden library(tidyverse) library(here) library(dslabs) library(grid) library(readr) library(RPostgreSQL) library(gridExtra) library(ggpubr) library(stringr) library(png) library(DT) library(ggplot2) In dit chunk heb ik de alle data van github geladen en daarna heb ik alle data wel een tidy gemaakt met behulp met verschillende functies om daarna alle drie data samen met elkaar toe te voegen. En worden alle data als csv en rds opgeslagd. ## Flu data laden flu_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/flu_data.csv&quot;, skip = 10) ## Rows: 659 Columns: 30 ## ── Column specification ─────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (29): Argentina, Australia, Austria, Belgium, Bolivia, Brazil, Bulgaria... ## date (1): Date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Laten we zien de inhoud van de eerste 10 regels van flu data datatable(flu_data, options = list(scrollx=TRUE, pageLength = 10)) ## Dengue data laden dengue_data &lt;- read_csv(&quot;https://raw.githubusercontent.com/DataScienceILC/tlsc-dsfb26v-20_workflows/main/data/dengue_data.csv&quot;, skip = 10) ## Rows: 659 Columns: 11 ## ── Column specification ─────────────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## dbl (10): Argentina, Bolivia, Brazil, India, Indonesia, Mexico, Philippines... ## date (1): Date ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Laten we zien de inhoud van de eerste 10 regels van dengue data datatable(dengue_data, options = list(scrollx=TRUE, pageLength = 10)) ## Gapminder data van de dslabs package gapminder_data &lt;- gapminder ## Laten we zien de inhoud van de eerste 10 regels van gapminder data datatable(gapminder_data, options = list(scrollx=TRUE, pageLength = 10)) #Data Tidy maken ##Flu data tidy flu_data_tidy &lt;- flu_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;flu_aantal&quot;) ## Verwijder de dag en de maand van de Date kolom, hernomen van de Date kolom naar Year en aanpassen van variabelen data typen flu_data_tidy$Date &lt;- str_sub(flu_data_tidy$Date, start = 1, end = 4) flu_data_tidy &lt;- rename(flu_data_tidy, year = Date) flu_data_tidy$country &lt;- as.factor(flu_data_tidy$country) flu_data_tidy$year &lt;- as.integer(flu_data_tidy$year) ## Laten we zien hoe ziet de data nu eruit datatable(flu_data_tidy, options = list(scrollx=TRUE, pageLength = 10)) ## Warning in instance$preRenderHook(instance): It seems your data is too big for ## client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html ## Dengue data tidy dengue_data_tidy &lt;- dengue_data %&gt;% pivot_longer(cols = -Date, names_to = &quot;country&quot;, values_to = &quot;dengue_aantal&quot;) ## Verwijder de dag en de maand van de Date kolom, hernomen van de Date kolom naar Year en aanpassen van variabelen data typen dengue_data_tidy$Date &lt;- str_sub(dengue_data_tidy$Date, start = 1, end = 4) dengue_data_tidy &lt;- rename(dengue_data_tidy, year = Date) dengue_data_tidy$year &lt;- as.integer(dengue_data_tidy$year) dengue_data_tidy$country &lt;- as.factor(dengue_data_tidy$country) ##Laten we zien hoe ziet de data nu eruit datatable(dengue_data_tidy, options = list(scrollx=TRUE, pageLength = 10)) # Opslaan alle drie dataframes als csv en rds bestanden ## Als csv bestanden write.csv(flu_data_tidy, file = here(&quot;data/flu.csv&quot;), row.names = FALSE) write.csv(dengue_data_tidy, file = here(&quot;data/dengue.csv&quot;), row.names = FALSE) write.csv(gapminder_data, file = here(&quot;data/gapminder_data.csv&quot;), row.names = FALSE) ## Als rds bestanden saveRDS(flu_data_tidy, file = here(&quot;data/flu_data.rds&quot;)) saveRDS(dengue_data_tidy, file = here(&quot;data/dengue_data.rds&quot;)) saveRDS(gapminder_data, file = here(&quot;data/gapminder_data.rds&quot;)) De volgende stap is het maken van nieuwe PostgreSQL database op DBeaver met dit code : create database workflowsdb. 6.1 Maak verbinding met de database con &lt;- dbConnect(RPostgres::Postgres(), dbname = “workflowsdb”, host = “localhost”, port = “5432”, user = “postgres”, password = “Fatima1996@”) ### SQL-scripts voor het schrijven van tabellen naar de database vanuit dataframes dbWriteTable(con, “flu_data_table”, flu_data_tidy, overwrite = TRUE) dbWriteTable(con, “dengue_data_table”, dengue_data_tidy, overwrite = TRUE) dbWriteTable(con, “gapminder_data_table”, gapminder_data, overwrite = TRUE) Disconnect van de database dbDisconnect(con) inhoud bekijken van flu_data_table select * from flu_data_table inhoud bekijken van dengue_data_table select * from dengue_data_table inhoud bekijken van gapminder_data_table select * from gapminder_data_table Bij onderstande figuur, het is de opgeslaagde SQL script sql_script &lt;- rasterGrob(as.raster(readPNG(here(&quot;Images/saved_sqlscript.png&quot;)))) # laat de figuur zien grid.arrange(sql_script) 6.2 Samenvoegende table van alle drie data De sql script van het samenvogen table van alle drie data is in onderstaande figuur samenvogende_sqlscript &lt;- rasterGrob(as.raster(readPNG(here(&quot;Images/samenvoegende_table.png&quot;)))) grid.arrange(samenvogende_sqlscript) Het laden van de samengevoegde table samengevoegde_table_data &lt;- read.csv(here(&quot;data/samenvoegende_table_202312111229.csv&quot;)) ## laten we de inhoud zien datatable(samengevoegde_table_data, options = list(scrollx=TRUE, pageLength = 10)) ## Warning in instance$preRenderHook(instance): It seems your data is too big for ## client-side DataTables. You may consider server-side processing: ## https://rstudio.github.io/DT/server.html 6.3 Drie statistieken met samenvogende table In dit project heb ik gegevens samengevoegd van verschillende tabellen en beschrijvende statistieken gegenereerd. Hieronder vindt u een overzicht van mijn aanpak en de resultaten. ## Symmary statistieken summary(samengevoegde_table_data) ## year country life_expectancy infant_mortality ## Min. :1960 Length:165216 Min. :13.20 Min. : 1.50 ## 1st Qu.:2005 Class :character 1st Qu.:72.10 1st Qu.: 13.40 ## Median :2008 Mode :character Median :74.50 Median : 15.40 ## Mean :2007 Mean :73.59 Mean : 21.98 ## 3rd Qu.:2012 3rd Qu.:75.60 3rd Qu.: 27.40 ## Max. :2016 Max. :83.90 Max. :276.90 ## NA&#39;s :1453 ## fertility population gdp continent ## Min. :0.840 Min. :3.124e+04 Min. :4.040e+07 Length:165216 ## 1st Qu.:2.000 1st Qu.:1.040e+07 1st Qu.:1.910e+10 Class :character ## Median :2.250 Median :4.254e+07 Median :3.979e+11 Mode :character ## Mean :2.516 Mean :8.640e+07 Mean :4.888e+11 ## 3rd Qu.:2.860 3rd Qu.:1.237e+08 3rd Qu.:6.999e+11 ## Max. :9.220 Max. :1.376e+09 Max. :1.174e+13 ## NA&#39;s :187 NA&#39;s :185 NA&#39;s :45706 ## region flu_aantal dengue_aantal ## Length:165216 Min. : 0.0 Min. :0.000 ## Class :character 1st Qu.: 153.0 1st Qu.:0.027 ## Mode :character Median : 236.0 Median :0.059 ## Mean : 442.6 Mean :0.102 ## 3rd Qu.: 543.0 3rd Qu.:0.118 ## Max. :10555.0 Max. :1.000 ## NA&#39;s :18353 NA&#39;s :27214 ## Visualisatie 1: africa_flu_aantal &lt;- samengevoegde_table_data %&gt;% select(year, country, continent, flu_aantal) %&gt;% filter(continent == &quot;Americas&quot;, !is.na(flu_aantal)) africa_flu_aantal_summary &lt;- africa_flu_aantal %&gt;% group_by(country) %&gt;% summarise(mean = mean(flu_aantal, na.rm = TRUE), stedv = sd(flu_aantal, na.rm= TRUE)) africa_flu_aantal_summary %&gt;% ggplot(aes(x = as.factor(country), y = mean, group = country, fill = country)) + geom_col() + labs() ## Visualisatie 2: canda_US_summary &lt;- africa_flu_aantal %&gt;% select(country,flu_aantal) %&gt;% filter(country %in% c(&quot;Canada&quot;, &quot;United States&quot;)) %&gt;% group_by(country) %&gt;% summarise(mean = mean(flu_aantal), stdev = sd(flu_aantal)) canda_US_summary %&gt;% ggplot(aes(x= country, y= mean, group= country, fill = country)) + geom_col() + geom_errorbar(aes(ymin=mean-stdev, ymax=mean+stdev), width=.2) + labs(title =&quot;gemiddelde flu aantal in Canada en in US&quot; , x=&quot;Country&quot;, y=&quot;Flu aantal gemiddeld&quot;) ## Visualisatie 3: life_ex_flu &lt;- samengevoegde_table_data %&gt;% select(year,continent, life_expectancy,flu_aantal) %&gt;% filter(continent == &quot;Europe&quot; ,!is.na(flu_aantal)) life_ex_flu %&gt;% group_by(year,continent) %&gt;% mutate(flu_gem = mean(flu_aantal, na.rm = TRUE)) %&gt;% ggplot(aes(x= year, y = flu_gem)) + geom_line(aes(colour = continent)) "],["data-managment-met-gruerrilla-analytics-structure.html", "7 Data managment met Gruerrilla analytics structure", " 7 Data managment met Gruerrilla analytics structure In dit opdracht heb ik een tree gemaakt voor DAUR2 mapje met behulp van Gruerrilla analytics structure fs::dir_tree(here::here(&quot;DAUR2&quot;)) ## C:/Users/Laptop/Downloads/Workflow-Portfolio/DAUR2 ## ├── Metagenomics ## │ ├── Data ## │ │ ├── Bracken_data ## │ │ │ ├── mock1.bracken ## │ │ │ ├── mock1.report ## │ │ │ ├── mock1_bracken_species.biom ## │ │ │ └── mock1_bracken_species.report ## │ │ ├── HU_waternet_MOCK1_composition.csv ## │ │ └── README.txt ## │ ├── README.txt ## │ ├── Results ## │ │ └── metagenomics_mock1_compostion.html ## │ └── Script ## │ └── metagenomics_mock1_compostion.Rmd ## └── RNAseq ## ├── Eindopdracht_RNAseq ## │ ├── Data ## │ │ ├── onecut_sampledata_OC3.csv ## │ │ ├── README.txt ## │ │ └── read_counts.rds ## │ ├── Results ## │ │ └── eindopdracht_onecut.html ## │ └── Script ## │ └── eindopdracht_onecut.Rmd ## └── Formatieveopdracht_RNAseq ## ├── Data ## │ ├── ipsc_sampledata.csv ## │ ├── README.txt ## │ └── read_counts.rds ## ├── Results ## │ └── formatieve_opdracht_RNAseq.html ## └── Script ## └── formatieve_opdracht_RNAseq.Rmd "],["r-package.html", "8 R Package", " 8 R Package Hier heb ik een R-pakket gemaakt genaamd CovidPackage. Dit pakket is ontwikkeld om te assisteren bij de analyse van COVID-19-gegevens. Elke functie binnen dit pakket heeft een specifieke rol bij het verwerken, analyseren of visualiseren van informatie die gerelateerd is aan COVID-19. En die R package kun je zien in mijn githup CovidPackage. # install.packages(&quot;devtools&quot;) devtools::install_github(&quot;Fatimadanawar/CovidPackage&quot;, build_vignettes = TRUE) Hier is een voorbeeld van een functie binnen deze R package. CovidPackage::plot_dagelijkse_gevallen(data_covid, &quot;Netherlands&quot;) Dagelijkse gevallen in Nederland "],["parameterized-covid-19-report.html", "9 Parameterized COVID-19 report", " 9 Parameterized COVID-19 report Het doel van deze hoofdstuk is het maken van een Rmarkdown-bestand met daarin een Parameterized COVID-19 report. De Rmd bevat drie parameters: - Het land country waarop het rapport van toepassing is - Het jaar year waarop de gerapporteerde gegevens betrekking hebben - De periode in maanden month waaropde rapport betrekking heeft Nadat deze Rmarkdown met parameters heeft geknit, worden twee plots gemaakt, één met covid-sterfgevall en één met covid-gevallen in het gegeven land, jaar en maand. De parameterized Rmarkdown is beschikbaar via github met behulp van webshot heb ik een screenshot gemaakt van de geparametriseerde Rmarkdown HTML die ik heb gemaakt, dus het zou gemakkelijker zijn om een voorbeeld van deze oefening te bekijken op github-pagina’s. Laten we zien in de parameterized Rmarkdown van de standaardinstelling is Nederland October 2022 # screenshot maken van html met behlup van functie webshot webshot::webshot(&quot;document/params.html&quot;, &quot;Images/params.png&quot;) Params testing: Om parameters te testen gebruiken we de volgende functie en kunnen we daar welke parameters willen en selecteren. #rmarkdown::render(&quot;document/params.Rmd&quot;, params = list(country = &quot;Belgium&quot;, year = 2020, month = &quot;April&quot;), output_file = &quot;belgium_2020_April&quot;) Hier kunnen we de resultaten van vorige stap zien als webshot webshot::webshot(&quot;document/belgium_2020_April.html&quot;, &quot;Images/belgium_2020_April.png&quot;) Nadat u de resultaten heeft gecontroleerd, kunt u zien dat de parameters werken en voor andere landen en data kunnen worden gebruikt. "],["slootwater-project.html", "10 Slootwater Project 10.1 Inleiding tot het Project", " 10 Slootwater Project In deze sectie zal ik kort een project introduceren gerelateerd aan metagenomica van slootwater, waaraan ik heb gewerkt tijdens mijn minor Data Science. Met deze introductie wil ik aantonen dat ik kennis heb van levenswetenschappelijk onderzoek en schrijftechnieken, evenals het gebruik van rmarkdown als communicatiemiddel. Citaten zijn opgenomen via de Zotero-webextensie. 10.1 Inleiding tot het Project Metagenomica is gedefinieerd als het sequencen van totaal genoom DNA dat direct is verkregen uit een specifiek milieu, zoals slootwater. Het doel is inzicht te krijgen in welke microbiele organismen aanwezig zijn en in welke verhoudingen. Vanuit de HogeSchool zijn er twee projecten die gebruik maken van metagenomics, waaronder het slootwaterproject. Het slootwaterproject betreft een metagenomica-analyse van lang leesbaar MinION-data. Hiervoor is een dataset gebruikt die is verkregen uit een slootwatermonster dat door ILC-studenten is geïsoleerd en bewerkt voor metagenomica-analyse op de MinION. De FastQ-bestanden zijn beschikbaar op de server. 10.1.1 Referenties Citaties zijn essentieel voor het begrijpen van metagenomics-analyse. De volgende referenties bieden waardevolle inzichten in metagenomics-classificatie met verschillende tools: KrakenUniq: Deze tool combineert snelle k-mer-gebaseerde classificatie met een efficiënt algoritme voor het beoordelen van de dekking van unieke k-mers die in elke soort in een dataset worden gevonden, en biedt zelfverzekerde en snelle metagenomics-classificatie (Breitwieser, Baker, and Salzberg 2018). Kraken 2: Een verbetering ten opzichte van Kraken 1, Kraken 2 vermindert het geheugengebruik aanzienlijk, waardoor grotere hoeveelheden referentiegenomische gegevens kunnen worden gebruikt, met behoud van hoge nauwkeurigheid en een vijfvoudige snelheidsverhoging (Derrick E. Wood, Lu, and Langmead 2019). Kraken: Een ultra snelle metagenomische sequentieclassificatietool die gebruikmaakt van exacte uitlijningen, waardoor snelle taxonomische classificatie van metagenomische sequentiegegevens mogelijk is (Derrick E. Wood and Salzberg 2014). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
